{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Classifier"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "import midi_encoder\n",
    "from train_classifier import *\n",
    "from train_generative import build_generative_model\n",
    "import plot_results\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [],
   "source": [
    "SAVE_CHECKPOINTS = os.path.join('trained')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [],
   "source": [
    "# variables\n",
    "vocabulary_path = os.path.join(SAVE_CHECKPOINTS, \"vocabulary_dict.json\")\n",
    "model_checkpoints = os.path.join(SAVE_CHECKPOINTS, 'sentiment_classifier.p')\n",
    "embedding_size = 256\n",
    "units = 512\n",
    "layers = 4\n",
    "batch_size = 1\n",
    "midis_path = os.path.join('vgmidi', 'labelled', 'midi')\n",
    "data_path = os.path.join('vgmidi', 'labelled', 'dataset', 'sentiment_labelled.csv')\n",
    "test_percentage = 0.2\n",
    "layer_index = 4"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [],
   "source": [
    "# load vocabulary\n",
    "with open(vocabulary_path) as input_file:\n",
    "    vocabulary = json.load(input_file)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [],
   "source": [
    "# vocabulary size\n",
    "vocabulary_size = len(vocabulary)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [],
   "source": [
    "# rebuild generative model from checkpoint\n",
    "generative_model = build_generative_model(vocabulary_size, embedding_size, units, layers, batch_size)\n",
    "generative_model.load_weights(tf.train.latest_checkpoint(SAVE_CHECKPOINTS))\n",
    "generative_model.build(tf.TensorShape([1, None]))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [],
   "source": [
    "# build datasets\n",
    "# x, y = build_dataset(midis_path, data_path, generative_model, vocabulary, layer_index)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [],
   "source": [
    "# np.savetxt('x_labelled_data.csv', x, delimiter=',')\n",
    "# np.savetxt('y_labelled_data.csv', y, delimiter=',')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [],
   "source": [
    "x_data = np.genfromtxt('x_labelled_data.csv', delimiter=',')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [],
   "source": [
    "y_data = np.genfromtxt('y_labelled_data.csv', delimiter=',')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=test_percentage)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [],
   "source": [
    "train_dataset = (x_train, y_train)\n",
    "test_dataset = (x_test, y_test)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\joana\\Desktop\\mecd\\HCAI\\emotone\\plot_results.py:37: RuntimeWarning: invalid value encountered in divide\n",
      "  coef = coef/norm\n"
     ]
    }
   ],
   "source": [
    "# train classifier\n",
    "sentiment_neurons, score = train_classifier_model(train_dataset, test_dataset)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total neurons used: 512\n",
      "Sentiment neurons:\n",
      "[511 510 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176\n",
      " 177 178 179 180 181 182 183 184 185 186 187 188 189 160 159 158 142 129\n",
      " 130 131 132 133 134 135 136 137 138 139 140 141 143 157 144 145 146 147\n",
      " 148 149 150 151 152 153 154 155 156 190 191 192 239 226 227 228 229 230\n",
      " 231 232 233 234 235 236 237 238 240 224 241 242 243 244 245 246 247 248\n",
      " 249 250 251 252 253 225 223 193 207 194 195 196 197 198 199 200 201 202\n",
      " 203 204 205 206 208 222 209 210 211 212 213 214 215 216 217 218 219 220\n",
      " 221 128 127 126  46  33  34  35  36  37  38  39  40  41  42  43  44  45\n",
      "  47  31  48  49  50  51  52  53  54  55  56  57  58  59  60  32  30  62\n",
      "  14   1   2   3   4   5   6   7   8   9  10  11  12  13  15  29  16  17\n",
      "  18  19  20  21  22  23  24  25  26  27  28  61  63 125 110  97  98  99\n",
      " 100 101 102 103 104 105 106 107 108 109 111  95 112 113 114 115 116 117\n",
      " 118 119 120 121 122 123 124  96  94  64  78  65  66  67  68  69  70  71\n",
      "  72  73  74  75  76  77  79  93  80  81  82  83  84  85  86  87  88  89\n",
      "  90  91  92 254 255 256 431 418 419 420 421 422 423 424 425 426 427 428\n",
      " 429 430 432 416 433 434 435 436 437 438 439 440 441 442 443 444 445 417\n",
      " 415 447 399 386 387 388 389 390 391 392 393 394 395 396 397 398 400 414\n",
      " 401 402 403 404 405 406 407 408 409 410 411 412 413 446 448 384 495 482\n",
      " 483 484 485 486 487 488 489 490 491 492 493 494 496 480 497 498 499 500\n",
      " 501 502 503 504 505 506 507 508 509 481 479 449 463 450 451 452 453 454\n",
      " 455 456 457 458 459 460 461 462 464 478 465 466 467 468 469 470 471 472\n",
      " 473 474 475 476 477 385 383 257 303 290 291 292 293 294 295 296 297 298\n",
      " 299 300 301 302 304 288 305 306 307 308 309 310 311 312 313 314 315 316\n",
      " 317 289 287 319 271 258 259 260 261 262 263 264 265 266 267 268 269 270\n",
      " 272 286 273 274 275 276 277 278 279 280 281 282 283 284 285 318 320 382\n",
      " 367 354 355 356 357 358 359 360 361 362 363 364 365 366 368 352 369 370\n",
      " 371 372 373 374 375 376 377 378 379 380 381 353 351 321 335 322 323 324\n",
      " 325 326 327 328 329 330 331 332 333 334 336 350 337 338 339 340 341 342\n",
      " 343 344 345 346 347 348 349   0]\n",
      "Model Accuracy: 40.0\n"
     ]
    }
   ],
   "source": [
    "# train classifier\n",
    "# results\n",
    "print(f'Total neurons used: {len(sentiment_neurons)}')\n",
    "print('Sentiment neurons:')\n",
    "print(sentiment_neurons)\n",
    "print(f'Model Accuracy: {score}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "hcai",
   "language": "python",
   "display_name": "hcai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
